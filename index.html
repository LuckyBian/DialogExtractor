<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="generator" content="Hugo 0.88.1" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
	<link rel="stylesheet" href=""https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
  
	<link rel="stylesheet" href="css/custom.css">
	<link rel="stylesheet" href="css/normalize.css">

	<title>GPTSOVITS2</title>
	<link href="css/bootstrap.min.css" rel="stylesheet">

  <style>
    * {
  box-sizing: border-box;
  overflow-x: auto; /* 防止水平滚动 */
}

    .audio-collection {
  width: fit-content; /* 或者设置一个具体的宽度，使容器适应缩小后的表格 */
  overflow-x: auto; /* 如果表格仍超出容器范围，允许横向滚动 */
  overflow-y: hidden;
}

.audio-small {
  width: 150px; /* 调整为适当的尺寸 */
  height: auto; /* 保持高度自动，以避免扭曲 */
}
  </style>

</head>
 
<div class="container">
  

	<header role="banner">
		
		
	</header>


<main role="main">
		<article itemscope itemtype="https://schema.org/BlogPosting">

            <h1 class="entry-title" itemprop="headline" align="center" ><font color="000093"  ><b>GPTSoVITS2</b></font></h1>
			
			<section itemprop="entry-text">
				<br>



<<!--摘要部分-->
<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded"> 

    <h2 id="abstract" style="text-align:center;"><font color="000093">Summary</font></h2>
    <style>
      .centered-link {
        text-align: center;
      }
    </style>
    
    <!-- <div class="centered-link">
      <a href="">CODE LINK</a>
    </div> -->
  
  
    <p style="text-align: justify; margin: 30px;"><font color="061E61">
      This project demonstrates the practical application of integrated Text-to-Speech (TTS) technologies to create virtual digital personas by adjusting and combining pre-existing models. The process involves sophisticated speech synthesis techniques to clone the timbre of target voices and articulate modeled text inputs effectively. The core technological approach includes the extraction and encoding of critical speech attributes such as pitch, emotion, and content within a latent discrete space, facilitating the production of audio outputs that embody specific emotional and rhythmic nuances tailored to individual voice characteristics through few-shot learning. Currently, this system supports both Chinese and English and is applied in innovative ways, including virtual character speeches and one-click, tone-preserving translations of cinematic dialogues. This practical deployment of adapted technologies showcases the ability to bridge gaps between existing TTS models and real-world applications, enhancing digital communication and interaction across linguistic and cultural barriers. Future enhancements will focus on refining the precision of text-to-audio outputs to capture more complex emotional expressions, further extending the utility and effectiveness of virtual personas.</font></p>
  </div>